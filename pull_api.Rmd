---
title: "Pull data from AirCasting API"
author: "Lyuou Zhang"
date: "4/7/2019"
output: html_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(httr)
library(jsonlite)
library(purrr)
library(stringr)
library(leaflet)
```

This `.Rmd` file pulls AirCasting data from API, clean and tidy them.

# Mobile sessions

Now we have fixed sessions and mobile sessions available. Ideally, we will use mobile sessions to do any analysis, and use fixed sessions as "knots" to validate the measurements. However, we don't know any models that can do that job. Anyway, I will pull data from mobile sessions first and see what they look like. I will also remove those that are obviously wrong.

I will follow Chris's method:  
  
*  Step 1: find usernames from AirCasting website under the MAPS session
*  Step 2: plug in usernames in the mobile session API call and get the IDs
*  Step 3: plug in IDs in the measurement ID API call and get the measurements in the form of points

## Step 1

Here are the usernames that I found on the AirCasting website:  
`NYCEJA`, `BLU 12`, `HabitatMap`, `BCCHE`, `scooby`, `Ana BCCHE`, `Ricardo Esparza`, `Tasha kk`, `lana`, `Marisa Sobel`, `Wadesworld18`, `El Puente 3`, `El Puente 4`, `El Puente 2`, `El Puente 1`, `mahdin`, `El Puente 5`, `Asemple`, `patqc`, `sjuma`

```{r}
usernames <- c('NYCEJA', 'BLU%2012', 'HabitatMap', 'BCCHE', 'scooby', 'Ana%20BCCHE', 'Ricardo%20Esparza', 'Tasha%20kk', 'lana', 'Wadesworld18', 'El%20Puente%204', 'El%20Puente%205', 'mahdin', 'Asemple', 'patqc')


```

write a function that takes one username from the username vector, plugs it into the  API call, and extracts the session IDs

```{r}
fetch_id <- function(name){
  api_call <- str_c('http://aircasting.org/api/sessions.json?page=0&page_size=500&q[measurements]=true&q[time_from]=0&q[time_to]=2552648500&q[usernames]=', name)
  api_pull <- jsonlite::fromJSON(api_call)
  user_id <- api_pull$streams$'AirBeam2-PM2.5'$id %>% 
    .[!is.na(.)]
  user_info <- api_pull %>% 
    do.call(data.frame, .) %>% 
    select(username, streams.AirBeam2.PM2.5) %>% 
    do.call(data.frame, .) %>% 
    select(username, streams.AirBeam2.PM2.5.id) %>% 
    filter(!is.na(streams.AirBeam2.PM2.5.id))

  user_info
}

pulled_ids <- map(usernames, fetch_id) %>% 
  bind_rows(.)
```

```{r}
# This function plugs each ID into the measurement API call and pulls data using that ID
pull_fun <- function(id_element){
  test_sess <- str_c("http://aircasting.org/api/realtime/stream_measurements.json/?end_date=2281550369000&start_date=0&stream_ids[]=",id_element) %>% 
    jsonlite::fromJSON(.) %>% 
    mutate(id = id_element) %>% 
    as_tibble()

  test_sess
}

# The output of pull_fun is a list. Take each element of the list and combine them into a tibble 
airbeam_data <- map(pulled_ids$streams.AirBeam2.PM2.5.id, pull_fun) %>% 
  do.call("bind_rows", .) %>% 
  inner_join(., pulled_ids, by = c('id' = 'streams.AirBeam2.PM2.5.id'))
```

# Data cleaning for mobile sessions

We did the following to clean the data:

1.  Seperated date and time and created a date variable 
2.  Removed the values that beyond the range (0, 250), and divided the data into two subsets: a subset with regular values and the other with extremely high values. We analyzed them seperately
3.  Removed or edited the latitude and longitude that were not in New York. 

```{r}
airbeam_data_tidy = airbeam_data %>% 
  separate(time, into = c("year", "month", "day"), sep = "-") %>% 
  separate(day, into = c("day", "time"), sep = "T") %>% 
  separate(time, into = c("hour", "min", "sec"), sep = ":") %>% 
  separate(sec, into = c("sec", "remove"), sep = "Z") %>% 
  select(-remove) %>% 
  mutate(
    date = str_c(year, month, day, sep = '-'),
    date = as.Date(date)
  ) %>% 
  filter(value > 0 & value < 1000) %>% 
  filter(latitude > 38 & longitude < -70) %>% 
  filter(latitude > 40 & longitude > -75)
```

```{r}
write.csv(airbeam_data_tidy, './data/airbeam_data_tidy.csv')



```


**Subset 1: regular measurements**

```{r}
airbeam_reg <- airbeam_data_tidy %>% 
  filter(value <= 50)
```

**Subset 2: extremely high values**

```{r}
airbeam_high <- airbeam_data_tidy %>% 
  filter(value > 50)
```

Our analysis willl primarily focus on the regular measurements. However, extremely high measurements are also useful, because they could help identify potential sources that cause peaks in PM2.5. These two subsets will be analyzed seperately.

# Visualization

Here's the updated version of visualization to test the plots for the final deliverable in static forms. A few notes:  

*  Time duration has to change over time
*  Because of the quantity of the data, they have to be reduced at least to 1-minute
*  Daily average and hourly average
*  Group by usernames?

Plot of Overall Data

```{r}
airbeam_data_tidy %>% 
  mutate(id = as.factor(id)) %>%
  ggplot(aes(x = date, y = value, color = id)) + geom_point() +
  labs(
    title = 'All data points, March 2018 - April 2019',
    x = 'Date',
    y = 'PM2.5, ug/m3'
  ) +
  theme(legend.position = "none")
```

Daily Average Plots

```{r}
airbeam_data_tidy %>% 
  mutate(date = as.factor(date)) %>% 
  group_by(date, id) %>% 
  mutate(daily_average = mean(value)) %>% 
  select(date, daily_average, id) %>% 
  distinct() %>% 
  ggplot(aes(x = date, y = daily_average, color = date)) + geom_point() +
  labs(
    title = 'Daily averages at each station, March 2018 - April 2019',
    x = 'Date',
    y = 'Average PM2.5, ug/m3'
  ) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90))
```

```{r}
airbeam_data_tidy %>% 
  mutate(date = as.factor(date)) %>% 
  group_by(date) %>% 
  mutate(daily_average = mean(value)) %>% 
  select(date, daily_average) %>% 
  distinct() %>% 
  ggplot(aes(x = date, y = daily_average)) + geom_point() + geom_line(group = 1) +
  labs(
    title = 'Overall daily averages, March 2018 - April 2019',
    x = 'Date',
    y = 'Average PM2.5, ug/m3'
  ) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90))
```

GEO

```{r}
location_reg <- airbeam_reg %>%
  mutate(latitude = round(latitude, 3),
         longitude = round(longitude, 3)) %>% 
  group_by(latitude, longitude) %>% 
  summarize(avg_pm = mean(value))
  

leaflet(data = location_reg) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addCircleMarkers(
    lat = ~latitude, lng = ~longitude,
    color = 'green',
    radius = 0.2
  )
```

Daily trends

```{r}
airbeam_data_tidy %>% 
  group_by(hour) %>% 
  summarize(hourly_average = mean(value)) %>%
  knitr::kable()
  
ggsave('all_data_points.jpeg')
```



